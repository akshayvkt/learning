{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a dataset for LLM fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Final fine-tuning should be done on a GPU, but rest the groundwork, including dataset preparation can be done locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (0.42.0)\n",
      "Requirement already satisfied: transformers in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (4.37.2)\n",
      "Requirement already satisfied: peft in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (0.8.2)\n",
      "Requirement already satisfied: accelerate in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (0.27.2)\n",
      "Requirement already satisfied: datasets in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (2.17.0)\n",
      "Requirement already satisfied: scipy in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (8.1.2)\n",
      "Requirement already satisfied: matplotlib in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (3.8.3)\n",
      "Requirement already satisfied: filelock in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from peft) (2.2.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: backcall in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: executing in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes transformers peft accelerate datasets scipy ipywidgets matplotlib huggingface wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Alpaca dataset can be downloaded at** https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alpaca_gpt4_data.json','r') as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52002"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Provide me a list of diseases that can be caused by smoking.',\n",
       " 'input': '',\n",
       " 'output': 'Sure! Here is a list of some of the diseases that can be caused by smoking: \\n\\n1. Lung cancer\\n2. Chronic obstructive pulmonary disease (COPD)\\n3. Heart disease\\n4. Stroke\\n5. Emphysema\\n6. Atherosclerosis\\n7. Peripheral artery disease\\n8. Esophageal cancer\\n9. Throat cancer\\n10. Oral cancer\\n11. Bladder cancer\\n12. Pancreatic cancer\\n13. Kidney cancer\\n14. Stomach cancer\\n15. Type 2 diabetes\\n16. Rheumatoid arthritis\\n17. Infertility\\n18. Chronic bronchitis\\n19. Cataracts\\n20. Gum disease and tooth loss\\n\\nIt is important to note that smoking can also weaken the immune system, making it harder for the body to fight off infections and diseases. Additionally, smoking can worsen existing health conditions and reduce the effectiveness of certain medications.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca[420]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pre-process this data so we can feed it into the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge**: `format_map` function: By using format_map method, you can feed our dictionary's values into a string in a straight-forward way by placing the keys inside curly brackets i.e., {}\n",
    "\n",
    "Here's an example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input stored in variable a. \n",
    "a = {'x':'John', 'y':'Wick'} \n",
    "\n",
    "# Use of format_map() function \n",
    "print(\"{x}'s last name is {y}\".format_map(a))\n",
    "\n",
    "print('Keanu Reeves had a new release called {x} {y}'.format_map(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `format_map` for our purpose to create two functions that can process a json and convert them into an LLM-friendly input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_without_input(row):\n",
    "    return(\"Below is an instruction that describes a task.\" \n",
    "           \"Write a response that appropriately completes the request. \\n\\n\"\n",
    "           \"###Instruction: \\n{instruction}\").format_map(row)\n",
    "\n",
    "def prompt_with_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pick out the correct noun from the following list.\n",
      "\n",
      "### Input:\n",
      "river, mountain, book\n",
      "\n",
      "### Response:\n",
      "\n",
      "Here is a different one\n",
      "\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request. \n",
      "\n",
      "###Instruction: \n",
      "Provide me a list of diseases that can be caused by smoking.\n"
     ]
    }
   ],
   "source": [
    "print(prompt_with_input(alpaca[426]))\n",
    "print('Now here is one without an input\\n')\n",
    "print(prompt_without_input(alpaca[420]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that combines the two functions above and produces prompts for our LLM based on whether there's an input in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_for_item(row):\n",
    "    if row['input'] == '':\n",
    "        return prompt_without_input(row)\n",
    "    else:\n",
    "        return prompt_with_input(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [create_prompt_for_item(row) for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0:\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request. \n",
      "\n",
      "###Instruction: \n",
      "Give three tips for staying healthy.\n",
      "\n",
      "\n",
      "Prompt 1:\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request. \n",
      "\n",
      "###Instruction: \n",
      "What are the three primary colors?\n",
      "\n",
      "\n",
      "Prompt 2:\n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the request. \n",
      "\n",
      "###Instruction: \n",
      "Describe the structure of an atom.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,prompt in enumerate(prompts[:3]):\n",
    "    print(f\"Prompt {i}:\")\n",
    "    print(prompt)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing to handle special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End-of-String (EOS) token:\n",
    "The EOS token is important because it's used to indicate to the LLM  that it should stop producing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = '</s>'\n",
    "outputs = [row['output'] + EOS_TOKEN for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge**: `zip` method is used to combine two iterables into a single iterable, where the items of two iterables are paired together are tuples.\n",
    "\n",
    "Example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('D', 2), ('B', 1), ('A', 4), ('C', 3)}\n"
     ]
    }
   ],
   "source": [
    "letter = [ \"A\", \"B\", \"C\", \"D\" ]\n",
    "id = [ 4, 1, 3, 2 ]\n",
    "\n",
    "# using zip() to map values\n",
    "mapped = zip(letter, id)\n",
    "\n",
    "print(set(mapped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [{'prompt':s,'output':t,'example':s+t} for s,t in zip(prompts,outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'Below is an instruction that describes a task.Write a response that appropriately completes the request. \\n\\n###Instruction: \\nGive three tips for staying healthy.',\n",
       "  'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>',\n",
       "  'example': 'Below is an instruction that describes a task.Write a response that appropriately completes the request. \\n\\n###Instruction: \\nGive three tips for staying healthy.1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>'},\n",
       " {'prompt': 'Below is an instruction that describes a task.Write a response that appropriately completes the request. \\n\\n###Instruction: \\nWhat are the three primary colors?',\n",
       "  'output': 'The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).</s>',\n",
       "  'example': 'Below is an instruction that describes a task.Write a response that appropriately completes the request. \\n\\n###Instruction: \\nWhat are the three primary colors?The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).</s>'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`meta-llama/Llama-2-7b-hf` repository requires access via HuggingFace, you can request it here: https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
    "\n",
    "Once you have access, you can either \n",
    "- Type `huggingface-cli login` in the Terminal and enter your access token, or\n",
    "- In the notebook, do\n",
    "`\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "`\n",
    "and enter your token\n",
    "\n",
    "\n",
    "To create a Huggingface token, log-in to Huggingface -> Settings -> Access Tokens -> New token -> Create a token with read access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f968fc56507e4c808c0f512cef1aa6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6c8adde0e64cf1b4d1efa1cde24b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669fe6a0d9d345caae29e78a4fb36c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89b38c826904665bb2e77262130685e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 20340, 29892, 338, 445, 1985, 29973]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testr = \"Wait, is this working?\"\n",
    "tokenizer.encode(testr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 20340, 29892, 338, 445, 1985, 29973, 2, 2, 2]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(testr, padding='max_length',max_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple tokens with index 2 that we see at the end are all EOS tokens, indicating the end of the sentence. There's 3 EOS tokens because we've set the `max_length=10`. If max_length is instead to 512, then you'd see a lot more EOS tokens, because we are padding the sentence to occupy the max_length length.\n",
    "\n",
    "You can see below that the 2nd token in our vocabulary is the EOS token, i.e., `</s>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['<s>'], tokenizer.vocab['</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get tensors instead of a list of token indices, set the parameter `return_tensors = pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 20340, 29892,   338,   445,  1985, 29973,     2,     2,     2]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(testr,padding='max_length',max_length=10,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset by creating a Train-Eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)  ##we shuffle the dataset using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:-1000]\n",
    "eval_dataset = dataset[-1000:]\n",
    "\n",
    "train_table = wandb.Table(dataframe = pd.DataFrame(train_dataset))\n",
    "eval_table = wandb.Table(dataframe = pd.DataFrame(eval_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/venkatakshaychintalapati/.netrc\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/venkatakshaychintalapati/Documents/GitHub/learning/LLM Finetuning/wandb/run-20240218_160956-zo1nzgnb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venkatakshay98/alpaca_ft/runs/zo1nzgnb' target=\"_blank\">bright-fuse-1</a></strong> to <a href='https://wandb.ai/venkatakshay98/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venkatakshay98/alpaca_ft' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venkatakshay98/alpaca_ft/runs/zo1nzgnb' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_ft/runs/zo1nzgnb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7257412c3e384e8cbc704fe291f2c4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.181 MB of 112.936 MB uploaded\\r'), FloatProgress(value=0.010454362529777994, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-fuse-1</strong> at: <a href='https://wandb.ai/venkatakshay98/alpaca_ft/runs/zo1nzgnb' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_ft/runs/zo1nzgnb</a><br/>Synced 6 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240218_160956-zo1nzgnb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two datasets, train and eval, have been uploaded as WandB tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining samples in dataset (called packing) to create longer sequences, reduce items to train, and make training more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 1024\n",
    "\n",
    "def pack(dataset,max_seq_len=1024):\n",
    "    # We are retrieving the list of tokens for each dataset sample here, and tkds_ids will be a list of these lists.\n",
    "    tkds_ids = tokenizer([s['example'] for s in dataset])['input_ids']\n",
    "\n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        # We are essentially flattening the list of lists here into a single list, with a EOS tokens demarcating consecutive examples.\n",
    "        # that's what the extend method does\n",
    "        all_token_ids.extend(tokenized_input + [tokenizer.eos_token_id])\n",
    "    \n",
    "    packed_ds = []\n",
    "    # What the below could be illustrated as: range(0, 121312, 1025)\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        \n",
    "        # One instance of below is all_token_ids[1025: 0 + 1025 + 1], which makes sense because we've used [0:1024] for input_ids and [1:1025] for output IDs\n",
    "        input_ids = all_token_ids[i : i + max_seq_len + 1]\n",
    "        ## If the length of this input_ids is equal to 1025, then take the items all the way from [:-1] meaning 0 to index 10\n",
    "        \n",
    "        # if len(input_ids) = 1025, which is the condition we want to use to pack our dataset's samples\n",
    "        # then we will prepare a pair of input_ids and labels.\n",
    "        # This also just means that, since we are accounting for when len != max_seq_len + 1, that means we're going to be\n",
    "        # dropping off and not including the very last sequence because its seq_len will be less than 1024.\n",
    "        if len(input_ids) == max_seq_len + 1:\n",
    "\n",
    "            packed_ds.append({'input_ids':input_ids[:-1],'labels': input_ids[1:]})\n",
    "    return packed_ds\n",
    "\n",
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing our preprocessed datasets in W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvenkatakshay98\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/venkatakshaychintalapati/Documents/GitHub/learning/LLM Finetuning/wandb/run-20240218_182056-404oauny</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venkatakshay98/alpaca_ft/runs/404oauny' target=\"_blank\">floating-goat-2</a></strong> to <a href='https://wandb.ai/venkatakshay98/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venkatakshay98/alpaca_ft' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venkatakshay98/alpaca_ft/runs/404oauny' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_ft/runs/404oauny</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e0c5d2e6ce4af788dd8886cdf16b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 129.401 MB uploaded\\r'), FloatProgress(value=6.271051116753411e-05, ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floating-goat-2</strong> at: <a href='https://wandb.ai/venkatakshay98/alpaca_ft/runs/404oauny' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_ft/runs/404oauny</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240218_182056-404oauny/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "# Dump packed datasets into jsonl to use it for finetuning\n",
    "save_jsonl(train_ds_packed, 'train_alpaca_packed.jsonl')\n",
    "save_jsonl(eval_ds_packed, 'eval_alpaca_packed.jsonl')\n",
    "\n",
    "# Create a WandB artifact\n",
    "\n",
    "packed_at = wandb.Artifact(\n",
    "    name = 'alpaca_packed',\n",
    "    type = 'dataset',\n",
    "    description = \"Alpaca dataset packed in sequences\",\n",
    "    metadata = {'max_seq_len':1024,'model_id':model_id}\n",
    ")\n",
    "\n",
    "packed_at.add_file('train_alpaca_packed.jsonl')\n",
    "packed_at.add_file('eval_alpaca_packed.jsonl')\n",
    "\n",
    "# Log the artifact to the project, call the job 'data_preprocess' \n",
    "\n",
    "with wandb.init(project='alpaca_ft', job_type='data_preprocess'):\n",
    "    wandb.log_artifact(packed_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
