{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning on Alpaca dataset using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading our preprocessed dataset from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkatakshaychintalapati/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from pathlib import Path \n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvenkatakshay98\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/venkatakshaychintalapati/Documents/GitHub/learning/LLM_Finetuning/wandb/run-20240220_193022-zxz9kdtr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/zxz9kdtr' target=\"_blank\">sweet-tiger-9</a></strong> to <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/zxz9kdtr' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/zxz9kdtr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact alpaca_packed:v0, 129.39MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:0.6\n"
     ]
    }
   ],
   "source": [
    "## Run this when you need to retrieve the dataset. I didn't run it because the files were locally available.\n",
    "run = wandb.init(project = 'alpaca_finetuning_2-20')\n",
    "\n",
    "## We're using the artifact we previously stored at this location in WandB\n",
    "artifact = run.use_artifact('venkatakshay98/alpaca_finetuning_2-20/alpaca_packed:v0', type='dataset')\n",
    "artifact_dir = Path(artifact.download())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 to load dataset: Use them as .jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def load_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/venkatakshaychintalapati/Documents/GitHub/learning/LLM_Finetuning/artifacts/alpaca_packed:v0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this if you need to download the datasets from a WANDB artifact directory\n",
    "train_ds_packed = load_jsonl(f\"{artifact_dir}/train_alpaca_packed.jsonl\")\n",
    "eval_ds_packed =load_jsonl(f\"{artifact_dir}/eval_alpaca_packed.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this if you have the files available locally for use and don't need to download\n",
    "train_ds_packed = load_jsonl('train_alpaca_packed.jsonl')\n",
    "eval_ds_packed = load_jsonl('eval_alpaca_packed.jsonl')\n",
    "\n",
    "len(train_ds_packed),len(eval_ds_packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The difference between the above where the dataset is a plain json versus below when we load this using load_dataset is that** the latter has many advantages such as fast loading, built-in map/filter methods, etc. Hence we use the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 to load data: Loading them uisng the Huggingface method `load_dataset` which converts them into a format more ideal for creating dataloaders and training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zxz9kdtr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d89f34cbb44ee58f2a7dab740ec288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-tiger-9</strong> at: <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/zxz9kdtr' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/zxz9kdtr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240220_193022-zxz9kdtr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zxz9kdtr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb67c3ebd1a9449e85e6fb303cf0f630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167940732816028, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/venkatakshaychintalapati/Documents/GitHub/learning/LLM_Finetuning/wandb/run-20240220_193325-49dz2qa4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/49dz2qa4' target=\"_blank\">glistening-fish-10</a></strong> to <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/49dz2qa4' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/49dz2qa4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact alpaca_packed:v0, 129.39MB. 2 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
      "Done. 0:0:0.6\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "run = wandb.init(project='alpaca_finetuning_2-20')\n",
    "artifact = run.use_artifact('venkatakshay98/alpaca_finetuning_2-20/alpaca_packed:v0', type='dataset') # Declares the artifact as an input to run\n",
    "artifact_dir = artifact.download() ## when we call .download() on the artifact, it downloads (gets) the contents locally.\n",
    "\n",
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We are using the HF method load_dataset which provides many advantages for dataset loading specific to training models on it.\n",
    "ds_packed = load_dataset(artifact_dir) \n",
    "\n",
    "train_ds_packed = ds_packed['train']\n",
    "eval_ds_packed = ds_packed['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 11105\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_packed,eval_ds_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 11105\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 222\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_packed ## using load_dataset method of HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieving our eval_dataset from WandB that we saved previously**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the jsonl into a list of dictionaries, the way we want to feed it into our validation loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def load_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename,'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading the `eval_dataset` that we created in `LLM_Finetuning/prepare_dataset_for_FT.ipynb`**\n",
    "\n",
    "This is not a packed dataset; because we want to run eval on it, so we don't want to fill up the sequence to `max_seq_len`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:49dz2qa4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b691e2dce226419697f03d22fac88492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.10986905142127117, max=1.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glistening-fish-10</strong> at: <a href='https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/49dz2qa4' target=\"_blank\">https://wandb.ai/venkatakshay98/alpaca_finetuning_2-20/runs/49dz2qa4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240220_193325-49dz2qa4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:49dz2qa4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85610bc257024e5aadb2b415fdd74d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168304632883519, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/venkatakshaychintalapati/Documents/GitHub/learning/LLM_Finetuning/wandb/run-20240220_193840-4yo6z26r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venkatakshay98/learning-LLM_Finetuning/runs/4yo6z26r' target=\"_blank\">thriving-monkey-12</a></strong> to <a href='https://wandb.ai/venkatakshay98/learning-LLM_Finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venkatakshay98/learning-LLM_Finetuning' target=\"_blank\">https://wandb.ai/venkatakshay98/learning-LLM_Finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venkatakshay98/learning-LLM_Finetuning/runs/4yo6z26r' target=\"_blank\">https://wandb.ai/venkatakshay98/learning-LLM_Finetuning/runs/4yo6z26r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "run = wandb.init()\n",
    "artifact = run.use_artifact('venkatakshay98/alpaca_finetuning_2-20/alpaca_dataset_for_evals:v2', type='dataset')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'prompt': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a cover letter applying to become a teacher.\\n\\n### Input:\\nYour resume\\n\\n### Response:\\n',\n",
       "   'output': \"Dear Hiring Manager,\\n\\nI am writing to apply for the position of a teacher, as advertised on your company's website. I have attached my Resume for your review.\\n\\nAs an experienced educator, I am passionate about teaching and empowering students to reach their full potential. My background in education includes a Bachelor's degree in Education, and I have had the opportunity to teach diverse student populations in both urban and suburban environments. In my previous role, I developed and taught engaging lesson plans that were tailored to the needs and interests of my students, and my lessons consistently resulted in increased student engagement and achievement.\\n\\nI pride myself on my ability to connect with my students and create a safe and welcoming environment for learning. My goal as a teacher is to instill a lifelong love of learning and to inspire my students to succeed in all aspects of their lives.\\n\\nThank you for considering my application.\\n\\nSincerely,\\nYour Name.</s>\",\n",
       "   'example': \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWrite a cover letter applying to become a teacher.\\n\\n### Input:\\nYour resume\\n\\n### Response:\\nDear Hiring Manager,\\n\\nI am writing to apply for the position of a teacher, as advertised on your company's website. I have attached my Resume for your review.\\n\\nAs an experienced educator, I am passionate about teaching and empowering students to reach their full potential. My background in education includes a Bachelor's degree in Education, and I have had the opportunity to teach diverse student populations in both urban and suburban environments. In my previous role, I developed and taught engaging lesson plans that were tailored to the needs and interests of my students, and my lessons consistently resulted in increased student engagement and achievement.\\n\\nI pride myself on my ability to connect with my students and create a safe and welcoming environment for learning. My goal as a teacher is to instill a lifelong love of learning and to inspire my students to succeed in all aspects of their lives.\\n\\nThank you for considering my application.\\n\\nSincerely,\\nYour Name.</s>\"},\n",
       "  {'prompt': 'Below is an instruction that describes a task.Write a response that appropriately completes the request. \\n\\n###Instruction: \\nWhat type of information should someone provide when filling out a job application?',\n",
       "   'output': \"When filling out a job application, it is important to provide accurate and complete information in all the required fields. Some of the key information you should provide may include: \\n\\n1. Personal Information: This includes your full name, contact details (telephone number, email address), and mailing address.\\n\\n2. Employment History: You should list all your previous jobs, including the employer's name, dates of employment, job title, and job responsibilities. \\n\\n3. Educational Background: Provide details of your academic qualifications, including the schools or colleges you attended, your major, and the dates you graduated.\\n\\n4. Skills and Qualifications: It's important to highlight your skills and qualifications that are relevant to the job you're applying for. This may include specific skills, such as proficiency in a certain computer program, or certifications.\\n\\n5. Professional References: Some job applications may ask for references, which are people who can vouch for your abilities and work experience. It's important to provide the name, job title, and contact information for each reference.\\n\\n6. Cover Letter and Resume: You may also be required to attach a cover letter and resume to your job application. Make sure these documents are up-to-date and tailored specifically to the job you are applying for.\\n\\nRemember to read the instructions carefully and provide all the information that the employer is asking for. Providing complete and accurate information can increase your chances of being called for an interview.</s>\",\n",
       "   'example': \"Below is an instruction that describes a task.Write a response that appropriately completes the request. \\n\\n###Instruction: \\nWhat type of information should someone provide when filling out a job application?When filling out a job application, it is important to provide accurate and complete information in all the required fields. Some of the key information you should provide may include: \\n\\n1. Personal Information: This includes your full name, contact details (telephone number, email address), and mailing address.\\n\\n2. Employment History: You should list all your previous jobs, including the employer's name, dates of employment, job title, and job responsibilities. \\n\\n3. Educational Background: Provide details of your academic qualifications, including the schools or colleges you attended, your major, and the dates you graduated.\\n\\n4. Skills and Qualifications: It's important to highlight your skills and qualifications that are relevant to the job you're applying for. This may include specific skills, such as proficiency in a certain computer program, or certifications.\\n\\n5. Professional References: Some job applications may ask for references, which are people who can vouch for your abilities and work experience. It's important to provide the name, job title, and contact information for each reference.\\n\\n6. Cover Letter and Resume: You may also be required to attach a cover letter and resume to your job application. Make sure these documents are up-to-date and tailored specifically to the job you are applying for.\\n\\nRemember to read the instructions carefully and provide all the information that the employer is asking for. Providing complete and accurate information can increase your chances of being called for an interview.</s>\"}],\n",
       " 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_path = 'eval_dataset.jsonl'\n",
    "\n",
    "eval_dataset = load_jsonl(eval_dataset_path)\n",
    "\n",
    "eval_dataset[:2], len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator ## This method simply collates batches of dict-like objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function default_data_collator in module transformers.data.data_collator:\n",
      "\n",
      "default_data_collator(features: List[transformers.data.data_collator.InputDataClass], return_tensors='pt') -> Dict[str, Any]\n",
      "    Very simple data collator that simply collates batches of dict-like objects and performs special handling for\n",
      "    potential keys named:\n",
      "    \n",
      "        - `label`: handles a single value (int or float) per object\n",
      "        - `label_ids`: handles a list of values per object\n",
      "    \n",
      "    Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs\n",
      "    to the model. See glue and ner for example of how it's useful.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn = default_data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x29000f810>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'labels']),\n",
       " tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128]),\n",
       " tensor([13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,  3300,\n",
       "          2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889, 14350,\n",
       "           263,  2933,   393,  7128,  2486]),\n",
       " torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys(),batch['input_ids'][0][:25], batch['labels'][0][:25], batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sidenote: Some context on techniques we'll be using in our training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Checkpointing\n",
    "\n",
    "**How does it work?**\n",
    "This helps us when the available GPU memory is low, for the trade-off of additional computation time because we're recalculating the gradients that were discarded.\n",
    "\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "*TL;DR*: Gradient checkpointing just means that during forward pass, we store activations of only certain layers (called checkpoints) and discard the activations of other layers in order to save memory. \n",
    "\n",
    "This doesn't affect the outcome because, during backward pass, when we need to calculate gradients w.r.t activations of layers which were not stored, we can just calculate these activations again with the checkpoints we stored. \n",
    "\n",
    "##### In further detail:\n",
    "\n",
    "- **During the forward pass**: In gradient checkpointing, not all intermediate activations are stored. For layers designated as \"checkpoints,\" activations are stored, but for others, they are not. When you reach layer 3, for example, you compute its activations based on the inputs from layer 2 as usual.\n",
    "- **Discarding activations**: After the activations for layer 3 are computed from those of layer 2, the activations of layer 2 can be discarded if layer 2 is not a checkpoint. This is where the memory savings come in. By not storing the activations for every layer, you reduce the overall memory footprint.\n",
    "- **During the backward pass**: When it's time to compute the gradients with respect to the weights of layer 2, you need the activations of layer 2 again. Since they were discarded, you recompute them by doing a localized forward pass starting from the last checkpoint before layer 2. This might be the inputs to layer 1, or the activations of layer 1 if that was designated as a checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automated Mixed Precision\n",
    "\n",
    "**Why is it used?**\n",
    "This technique is again used to lower memory requirements and consequently speed up training, \n",
    "\n",
    "**How does it work?**\n",
    "By means of using a mix of precision when performing matmul in forward and backward pass. This means that some of the computations are performed in single-precision (aka 32-bit) and some in half-precision (16-bit). \n",
    "\n",
    "AMP can identify which parts of computation can be performed with which precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient accumulation:\n",
    "\n",
    "**Why is it used?**\n",
    "When performing backpropogation, we generally calculate the gradient of loss w.r.t weights and biases for entire batch at once, but this can be computationally expensive. By using gradient accumulation, we can reduce memory requirements as well, albeit with an increase in training time.\n",
    "\n",
    "**How does it work?**\n",
    " Instead of performing gradient calculation for an entire batch at once, we instead do this in parts, where we divide the batch into n parts, and calculate each part's gradient. We do not perform backprop yet, we continue to calculate gradient of the other parts, accumulate them to the existing gradient.\n",
    "\n",
    " Once the gradient of all these parts of a batch are complete, then we use that accumulated gradient to perform backpropogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 42\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = artifact.metadata['max_seq_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1389"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 32 // batch_size\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id = 'meta-llama/Llama-2-7b-hf',\n",
    "    dataset_name = 'alpaca-gpt4',\n",
    "    precision = 'bf16', # faster than fp16 apparently, also provides better precision and range of numbers\n",
    "    n_freeze = 24, # Frozen layers = layers not trained. Here, we are freezing 24 of 32 layers in Llama7b\n",
    "    lr = 2e-4,\n",
    "    n_eval_samples = 10, # Number of samples to generate on validation\n",
    "    max_seq_len = max_seq_len, ## The max_seq_len is mentioned in the metadata for our dataset \n",
    "    epochs = 3,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    batch_size=batch_size,\n",
    "    log_model = True, # uploading model to WandB\n",
    "    mom=0.9, #optimizer parameter\n",
    "    gradient_checkpointing = True,\n",
    "    freeze_embed=True, #keep the embeddings frozen\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We will train for {config.total_train_steps} steps and evaluate every epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are we dividing `config.epochs * len(train_dataloader)` by `config.gradient_accumulation_steps`?**\n",
    "\n",
    "Lets explain this with what gradient accumulation does. Let's say you're trying to use a batch size of 20 but that would lead to *GPU out of memory* error. We want to avoid this, but still we do not want to use a smaller batch size, because with a smaller batch size, it can create too much noise during training because of its batch size.\n",
    "\n",
    "Hence, to \n",
    "- stay within GPU memory, and \n",
    "- still effectively implement a larger batch size,\n",
    "\n",
    "we use something called **Gradient accumulation**. With this, we do not perform backprop after every single batch. Instead, we perform forward pass for a batch, calculate gradients w.r.t loss, store them, then do the same for more batches (the number depends on the `gradient_accumulation_steps` we choose). Once we accumulate these gradients, we then perform backprop using this to update our parameters.\n",
    "\n",
    "**Example**: That's why, if there's 100 batches, in the normal case, 1 epoch should have 100 training steps. *But* if we use gradient accumulation (say steps = 4), then we'll have only 25 training steps, because we're performing backprop only once every 4 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a pre-trained model with the configuration params set above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f716afe1b0464081ff09f2cac1fc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434c62cc72ad4801bea769235fa90541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b6ee47afae496591d245d5e5890233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54976a2882334159b6af0bdd261f3be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     config\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m      3\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m     trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     low_cpu_mem_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m      7\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3850\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3841\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3842\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3843\u001b[0m     (\n\u001b[1;32m   3844\u001b[0m         model,\n\u001b[1;32m   3845\u001b[0m         missing_keys,\n\u001b[1;32m   3846\u001b[0m         unexpected_keys,\n\u001b[1;32m   3847\u001b[0m         mismatched_keys,\n\u001b[1;32m   3848\u001b[0m         offload_index,\n\u001b[1;32m   3849\u001b[0m         error_msgs,\n\u001b[0;32m-> 3850\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3851\u001b[0m         model,\n\u001b[1;32m   3852\u001b[0m         state_dict,\n\u001b[1;32m   3853\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3854\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3855\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3856\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3857\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3858\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3859\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3860\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3861\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3862\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3863\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3864\u001b[0m         is_quantized\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES),\n\u001b[1;32m   3865\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3866\u001b[0m     )\n\u001b[1;32m   3868\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3869\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:4284\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4280\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4281\u001b[0m                         model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4282\u001b[0m                     )\n\u001b[1;32m   4283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4284\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4285\u001b[0m             model_to_load,\n\u001b[1;32m   4286\u001b[0m             state_dict,\n\u001b[1;32m   4287\u001b[0m             loaded_keys,\n\u001b[1;32m   4288\u001b[0m             start_prefix,\n\u001b[1;32m   4289\u001b[0m             expected_keys,\n\u001b[1;32m   4290\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4291\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4292\u001b[0m             offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[1;32m   4293\u001b[0m             state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[1;32m   4294\u001b[0m             state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[1;32m   4295\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   4296\u001b[0m             is_quantized\u001b[38;5;241m=\u001b[39mis_quantized,\n\u001b[1;32m   4297\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[1;32m   4298\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4299\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4300\u001b[0m         )\n\u001b[1;32m   4301\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:805\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    802\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized:\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mint8, torch\u001b[38;5;241m.\u001b[39muint8) \u001b[38;5;129;01mand\u001b[39;00m is_quantized:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# handling newly quantized weights and loaded quantized weights\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# edit the param.dtype restrictions and is_quantized condition when adding new quant methods\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     quantized_stats \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/utils/modeling.py:384\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    382\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 384\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map = 0,\n",
    "    trust_remote_code = True,\n",
    "    low_cpu_mem_usage = True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    use_cache = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, 1000000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## apparently this notation works\n",
    "type(1_000_000), 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainable parameters**: The parameters (weights and biases) in a model which can be updated during training thru backprop.\n",
    "\n",
    "**Non-trainable parameters**: This can be the parameters that we decide to keep frozen during training, or the layers of the model are kept as-is (when we add new layers on top that we train, in transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the number of parameters for the model\n",
    "\n",
    "def param_count(model):\n",
    "    params = sum([p.numel() for p in model.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\") ## M refers for million\n",
    "    return params, trainable_params \n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freezing the model to save memory\n",
    "\n",
    "Because training LLMs is expensive and GPUs have memory constraints. So in this specific case, we're freezing 24 of the 32 layers in Llama2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m n_freeze \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## Freezing all layers and params\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters(): param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m## Unfreezing the head of the model aka the final layer which outputs predictions\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mparameters(): param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "n_freeze = 24\n",
    "\n",
    "## Freezing all layers and params\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "\n",
    "## Unfreezing the head of the model aka the final layer which outputs predictions\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "\n",
    "## Unfreezing the layers 24 to end because they're the ones we want to train\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional reduction in memory requirements through freezing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we now also use gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\":False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking again what the number of params are\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizer**: An algorithm that updates the model's weights based on the gradients of the loss function with respect to those weights, with the goal to minimize the loss, and thereby improve the model's performance on the task.\n",
    "\n",
    "**Scheduler**: A scheduler aka learning rate scheduler is used to determine how to adjust the learning rate of the optimizer through the process of training a neural network.\n",
    "\n",
    "**Learning rate**: The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network with respect to the loss gradient.\n",
    "\n",
    "Both together work to train the neural network to perform better on the intended task(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr = config.lr, betas = (0.9,0.99),eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps = config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps//10\n",
    ")\n",
    "\n",
    "def loss_fn(x,y):\n",
    "    ## Cross entropy loss\n",
    "    return torch.nn.functional.cross_entropy(x.view(-1,x.shape[-1],y.view(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `eps` value, aka epsilon term, is a very small number added to the denominator of the update step of Adam, in order to ensure that we don't end up diving by zero. It is a numerical stability parameter.\n",
    "\n",
    "The `get_cosine_schedule_with_warmup` means that the LR starts at zero at the beginning of training, updates linearly till it hits the learning_rate prescribed by us when initializing the model, and it achieves it following the `num_warmup_steps` steps we prescribed.\n",
    "\n",
    "Once it hits the LR value, then it starts decreasing to zero, following a cosine schedule (meaning it gradually decreases following a part of the cosine curve from the max value to zero.)\n",
    "\n",
    "**Why use `get_cosine_schedule_with_warmup`**?\n",
    "This combination of initial warmup and the subsequent decrease helps the training with the need for larger LR in the initial steps for faster convergence, followed by the need for smaller LR later in training to perform precise adjustments to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, torch.Size([120, 7]), torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(4,5,6,7)\n",
    "k.shape[-1], k.view(-1,k.shape[-1]).shape,batch['labels'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model during training\n",
    "\n",
    "The `generate` function is used for inference to run predictions using the model at different steps of training, to visually see what the model is outputting. We can grab the default values for sampling parameters from the GenerationConfig and pass the corresponding model_id, which will grab the defaults for parameters like temperature, top p, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GenerationConfig` is a class that holds a configuration for a generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "\n",
    "def generate(prompt, max_new_tokens = 100, gen_config = gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors = 'pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt,\n",
    "                                max_new_tokens = max_new_tokens,\n",
    "                                generation_config = gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "## We are creating a test_config here so as to use it for eval.\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens = 256,\n",
    "    gen_config = gen_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prompt_table` below creates a table so we can view our model's predictions against GPT-4's output and assess how well it has learnt post-finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_table(examples, log=False, table_name = 'predictions'):\n",
    "    table = wandb.table(columns = ['prompt','generation','concat',\n",
    "                                   'output','max_new_tokens','temperature','top_p'])\n",
    "    \n",
    "    for example in tqdm(examples, leave=False):\n",
    "        prompt,gpt4_output = example['prompt'], example['output']\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, gpt4_output,test_config.max_new_tokens,\n",
    "                       test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    \n",
    "    if log:\n",
    "        ## We will use log=True only when running eval, as we set log=False for train time\n",
    "        wandb.log({table_name:table})\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_gpu` is a function to essentially move all the values (tensors) for all the keys of the dict to a GPU, in preparation for training our model on these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gpu(tensor_dict):\n",
    "    return {k:v.to('cuda') for k,v in tensor_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    \"Simple accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.correct = 0.\n",
    "    def update(self, logits, labels):\n",
    "        # calculating the predictions by applying argmax on logits.\n",
    "        predictions, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        correct = (logits == labels).sum()\n",
    "\n",
    "        ## here we append the number of items we ran predictions on\n",
    "        self.count += len(predictions)\n",
    "        ## here we append how many we got right of these predictions\n",
    "        self.correct += correct\n",
    "        ## this will be the updated accuracy\n",
    "        return correct/len(predictions) \n",
    "    # once the update function runs over all batches of our epoch, then we return the accuracy\n",
    "    # using the compute function\n",
    "    def compute(self):\n",
    "        return self.correct / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() ##this just indicates we'll not calculate gradients aka not perform backprop\n",
    "def validate():\n",
    "    model.eval() # putting the model in eval mode\n",
    "    eval_acc = Accuracy()\n",
    "    loss, total_steps = 0., 0\n",
    "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
    "        pbar.set_description(f\"doing validation\")\n",
    "        batch = to_gpu(batch)\n",
    "        total_steps += 1\n",
    "        with torch.amp.autocast('cuda',dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss += loss_fn(out.logits, batch['labels']) # you could use out.loss and not shift the dataset\n",
    "        # after every batch, we update the number of correctly predicted items and number of items into the accuracy function.\n",
    "        eval_acc.update(out.logits, batch['labels'])\n",
    "\n",
    "    # because we complete the evaluation over all batches, we now log results at the end\n",
    "    wandb.log({'eval/loss':loss.item()/total_steps,\n",
    "               'eval/accuracy':eval_acc.compute()})\n",
    "    \n",
    "    prompt_table(eval_dataset[:config.n_eval_samples],log=True)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying it all together: the model loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"alpaca_finetuning_2-20\", # the project I am working on\n",
    "           tags=[\"baseline\",\"7b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "for epoch in tqdm(range(config.epochs)):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "    validate()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the model checkpoint at the end\n",
    "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Eval Dataset evaluation\n",
    "\n",
    "Let's log a table with model predictions on the eval_dataset (or at least the 250 first samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=\"alpaca_ft\", # the project we are working on\n",
    "           job_type=\"eval\",\n",
    "           config=config): # the hyperparameters we want to keep track of\n",
    "    model.eval();\n",
    "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
