{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text corpus into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'algorithms',\n",
       " 'that',\n",
       " 'improve',\n",
       " 'automatically',\n",
       " 'through',\n",
       " 'experience',\n",
       " 'it',\n",
       " 'is',\n",
       " 'seen',\n",
       " 'as',\n",
       " 'a',\n",
       " 'subset',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'build',\n",
       " 'a',\n",
       " 'mathematical',\n",
       " 'model',\n",
       " 'based',\n",
       " 'on',\n",
       " 'sample',\n",
       " 'data',\n",
       " 'known',\n",
       " 'as',\n",
       " 'training',\n",
       " 'data',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'make',\n",
       " 'predictions',\n",
       " 'or',\n",
       " 'decisions',\n",
       " 'without',\n",
       " 'being',\n",
       " 'explicitly',\n",
       " 'programmed',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'are',\n",
       " 'used',\n",
       " 'in',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'email',\n",
       " 'filtering',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'where',\n",
       " 'it',\n",
       " 'is',\n",
       " 'difficult',\n",
       " 'or',\n",
       " 'infeasible',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'conventional',\n",
       " 'algorithms',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'the',\n",
       " 'needed',\n",
       " 'tasks']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a mapping from tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'study': 0,\n",
       " 'seen': 1,\n",
       " 'so': 2,\n",
       " 'conventional': 3,\n",
       " 'through': 4,\n",
       " 'or': 5,\n",
       " 'without': 6,\n",
       " 'model': 7,\n",
       " 'wide': 8,\n",
       " 'perform': 9,\n",
       " 'to': 10,\n",
       " 'tasks': 11,\n",
       " 'where': 12,\n",
       " 'difficult': 13,\n",
       " 'is': 14,\n",
       " 'experience': 15,\n",
       " 'develop': 16,\n",
       " 'data': 17,\n",
       " 'algorithms': 18,\n",
       " 'automatically': 19,\n",
       " 'build': 20,\n",
       " 'intelligence': 21,\n",
       " 'mathematical': 22,\n",
       " 'a': 23,\n",
       " 'variety': 24,\n",
       " 'it': 25,\n",
       " 'needed': 26,\n",
       " 'based': 27,\n",
       " 'known': 28,\n",
       " 'predictions': 29,\n",
       " 'such': 30,\n",
       " 'the': 31,\n",
       " 'learning': 32,\n",
       " 'being': 33,\n",
       " 'artificial': 34,\n",
       " 'training': 35,\n",
       " 'explicitly': 36,\n",
       " 'and': 37,\n",
       " 'on': 38,\n",
       " 'of': 39,\n",
       " 'machine': 40,\n",
       " 'computer': 41,\n",
       " 'make': 42,\n",
       " 'that': 43,\n",
       " 'infeasible': 44,\n",
       " 'email': 45,\n",
       " 'applications': 46,\n",
       " 'order': 47,\n",
       " 'programmed': 48,\n",
       " 'do': 49,\n",
       " 'sample': 50,\n",
       " 'filtering': 51,\n",
       " 'decisions': 52,\n",
       " 'used': 53,\n",
       " 'as': 54,\n",
       " 'subset': 55,\n",
       " 'improve': 56,\n",
       " 'vision': 57,\n",
       " 'are': 58,\n",
       " 'in': 59}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the 1HE matrices for inputs and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `yield from` itself starts a inner loop for each of the iterables, and yields the items of that iterable one-by-one. So the complexity here can be simplified to O(mxn) where m is number of iterables and n is # of items in each loop, or further simplified to O(T) where T is the total number of elements when we count all items from all iterables.\n",
    "\n",
    "Think of the concat function as running a for loop within a for loop to iterate through a list of lists, processing each element within a list exactly one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(*iterables):\n",
    "    for iterable in iterables:\n",
    "        yield from iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "list1 = [1, 2, 3]\n",
    "list2 = [4, 5, 6]\n",
    "list3 = [7, 8, 9]\n",
    "\n",
    "# Using the concat function to combine the lists\n",
    "combined = concat(list1, list2, list3)\n",
    "\n",
    "# Printing all elements in the combined sequence\n",
    "for element in combined:\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for 1HE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(id,vocab_size):\n",
    "    res = [0]*vocab_size\n",
    "    res[id] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens = len(tokens)\n",
    "n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are we doing below?\n",
    "- We create two sets of 1HE arrays, one array containing the 1HE arrays for index of our inputs in the vocab, and the other array containing the 1HE arrays for indices of our values (aka the tokens within the window of our input).\n",
    "\n",
    "#### How do we do this? \n",
    "- We iterate through all our tokens first\n",
    "\n",
    "#### Things to keep in mind\n",
    "- A token's position in the list of tokens is not the same as its position in the vocab list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokens,word_to_id,window_size):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    token_len = len(tokens)\n",
    "    vocab_size = len(word_to_id)\n",
    "    for index in range(token_len):\n",
    "        ## Create a list containing i and elements within its window\n",
    "        window = concat(range(max(0,index-window_size),index),range(index,min(token_len,index+window_size+1)))\n",
    "\n",
    "        for value_index in window:\n",
    "            if index==value_index:\n",
    "                ## we are skipping when i==j because a value can't be its own input\n",
    "                ## values are items in the window that are before/after the item, but not itself\n",
    "                continue\n",
    "            # X are inputs, y are values\n",
    "            # For first and last token, there's only two values\n",
    "            # For second and penultimate token, there's 3 values\n",
    "            # For the rest 80 tokens, there's 4 values, i.e., 2 before them and 2 after\n",
    "            # These add up to 2*2 + 3*2 + 4*80 which is equal to 330, the shape[0] of X and Y\n",
    "            # 60 aka the shape[1] of X and y is the size of our vocab\n",
    "            X.append(one_hot_encode(word_to_id[tokens[index]],vocab_size))\n",
    "            y.append(one_hot_encode(word_to_id[tokens[value_index]],vocab_size))\n",
    "\n",
    "    return np.asarray(X),np.asarray(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_training_data(tokens,word_to_id,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening in this neural network and its two layers?\n",
    "\n",
    "- We feed the 1HE matrix of our sentence with each row being the word's 1HE representation.\n",
    "\n",
    "**First layer**: Multiplying with this weight matrix is what produces our embedding matrix.\n",
    "- This is then matmul'd with a weight matrix which converts this sparse 1HE matrix into a dense embedding matrix, where each row is that token's embedding vector.\n",
    "- Essentially, this first weight matrix is an embedding look-up table, where each row is a token's enbedding vector and the # of columns is the dimension of the embedding space.\n",
    "\n",
    "**Second layer**: Why do we multiple with a second weight matrix?\n",
    "- This multiplication is to convert our embedding matrix into a matrix which contains the logits for its relation to other tokens in our embedding space. Let's call this output matrix B.\n",
    "- When we then apply softmax on top on this output B, that's when we get our final output matrix, where each row contains the probabilities for different tokens, with each probability indicating how related that token is to our input token (aka how contextually related is this to our input token)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(vocab_size,n_embedding):\n",
    "\n",
    "    model = {\n",
    "        'w1': np.random.randn(vocab_size,n_embedding),\n",
    "        'w2': np.random.randn(n_embedding,vocab_size)\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
